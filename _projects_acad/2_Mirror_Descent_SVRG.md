---
title: "Mirror Descent and Stochastic Variance Reduced Gradients"
collection: projects_acad
type: "Optimization Algorithms"
permalink: /projects/projects_acad/2_Mirror_Descent_SVRG
venue: "E1-260, Optimization for ML, IISc"
date: 2021-12-01
location: None
---

In mirror descent, we replace the commonly use Euclidean norm with the Bregman divergence. The implementation of Mirror descent is available [here](https://github.com/mainak-biswas1999/Academic_Projects/tree/main/Mirror%20Descent%20SVRG).

Normal SGD, mini-batch gradient descent is seen to converge slower due to their variance. SVRG is an stochastic gradient descent algorithm which can be mathematically show to have lower variance than the aforesaid algorithms and is also seen to perform better. [This](https://github.com/mainak-biswas1999/Academic_Projects/tree/main/Mirror%20Descent%20SVRG) contains the implementation of SVRG.
